{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is XGBoost?\n",
    "XGBoost is an incredibly popular machine learning library for good reason. It was developed originally as a C++ command-line application. After winning a popular machine learning competition, the package started being adopted within the ML community. As a result, bindings, or functions that tapped into the core C++ code, started appearing in a variety of other languages, including Python, R, Scala, and Julia. We will cover the Python API in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes XGBoost so popular?\n",
    "* Its speed and performance (Because the core XGBoost algorithm is parallelizable, it can harness all of the processing power of modern multi-core computers. Furthermore, it is parallelizable onto GPU's and across networks of computers, making it feasible to train models on very large datasets on the order of hundreds of millions of training examples)\n",
    "\n",
    "* It consistently outperforms almost all other single-algorithm methods in machine learning competitions and has been shown to achieve state-of-the-art performance on a variety of benchmark machine learning datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost: Fit/Predict\n",
    "It's time to create your first XGBoost model! As Sergey showed you in the video, you can use the scikit-learn `.fit()` / `.predict()` paradigm that you are already familiar to build your XGBoost models, as the `xgboost` library has a scikit-learn compatible API!\n",
    "\n",
    "Here, you'll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. It has been pre-loaded for you into a DataFrame called `churn_data` - explore it in the Shell!\n",
    "\n",
    "Your goal is to use the first month's worth of data to predict whether the app's users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you'll split the data into training and test sets, fit a small `xgboost` model on the training set, and evaluate its performance on the test set by computing its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import xgboost\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl (Specify n_estimators to be 10 estimators and an objective of 'binary:logistic')\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* constructed iteratively (one decision at a time)\n",
    " - until a stopping criterion is met\n",
    "* indivdual trees tend to overfit (high variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART (Classification and Regression Trees)\n",
    "* Each leaf always contains a real-valued score\n",
    "* Can be converted to categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees\n",
    "Your task in this exercise is to make a simple decision tree using scikit-learn's `DecisionTreeClassifier` on the `breast cancer` dataset that comes pre-loaded with scikit-learn.\n",
    "\n",
    "This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\n",
    "\n",
    "We've preloaded the dataset of samples (measurements) into `X` and the target values per tumor into `y`. Now, you have to split the complete dataset into training and testing sets, and then train a `DecisionTreeClassifier`. You'll specify a parameter called max_depth. Many other parameters can be modified within this model, and you can check all of them out [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is boosting?\n",
    "* Concept that can be applied to a set of machine learning models\n",
    "* Ensemble meta-algorithm used to convert many weak learners into a strong learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How boosting is accomplished\n",
    "- iteratively learning a set of weak models on subsets of the data \n",
    "- weighting each of their predictions according to each weak learner's performance\n",
    "- combine all of the weak learners' predictions multiplied by their weights to obtain a single final weighted prediction\n",
    "\n",
    "... that is much better than any of the individual predictions themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring accuracy\n",
    "\n",
    "You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a `DMatrix`.\n",
    "\n",
    "In the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix. So, that's what you will do here before running cross-validation on churn_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the DMatrix from X and y: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring AUC\n",
    "Now that you've used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the `metrics` parameter of `xgb.cv()`.\n",
    "\n",
    "Your job in this exercise is to compute another common metric used in binary classification - the area under the curve (`\"auc\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should I use XGBoost?\n",
    "* large number of training samples (best if more than 1000 training samples and less than 100 features, but if number of features < number of training samples it should be fine - I have doubts, it depends)\n",
    "* mixture of categorical and numeric features (or numeric features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When NOT to use XGBoost?\n",
    "* for very small datasets\n",
    "* for image recognition, computer vision and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective (loss) functions and base learners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function names in xgboost:\n",
    "    * reg:linear -> reg:squarederror - use for regression problems \n",
    "    * reg:logistic - use for classification problems when you want just a decision, not a probability \n",
    "    * binary:logistic - use when you want probability rather than just a decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base learners and why we need them\n",
    "* XGBoost involves creating a meta-model that is composed of many individual models that combine to give an final prediction\n",
    "* Individual models = base learners \n",
    "* Want base learners that when combined create final prediction that is non-linear \n",
    "* Each base learner should be good at distinguishing or predicting different parts of the dataset \n",
    "* Two kinds of base learners: tree and linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees as base learners\n",
    "It's now time to build an XGBoost model to predict house prices - not in Boston, Massachusetts, as you saw in the video, but in Ames, Iowa! This dataset of housing prices has been pre-loaded into a DataFrame called df. If you explore it in the Shell, you'll see that there are a variety of features about the house and its location in the city.\n",
    "\n",
    "In this exercise, your goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so you don't have to specify that you want to use trees here with `booster=\"gbtree\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ames_housing_trimmed_processed.csv\")\n",
    "y = df[['SalePrice']]\n",
    "X = df.drop('SalePrice', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalePrice\n",
       "0     208500\n",
       "1     181500\n",
       "2     223500\n",
       "3     140000\n",
       "4     250000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Remodeled</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>...</th>\n",
       "      <th>BldgType_TwnhsE</th>\n",
       "      <th>HouseStyle_1.5Unf</th>\n",
       "      <th>HouseStyle_1Story</th>\n",
       "      <th>HouseStyle_2.5Fin</th>\n",
       "      <th>HouseStyle_2.5Unf</th>\n",
       "      <th>HouseStyle_2Story</th>\n",
       "      <th>HouseStyle_SFoyer</th>\n",
       "      <th>HouseStyle_SLvl</th>\n",
       "      <th>PavedDrive_P</th>\n",
       "      <th>PavedDrive_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   Remodeled  GrLivArea  BsmtFullBath  BsmtHalfBath  ...  BldgType_TwnhsE  \\\n",
       "0          0       1710             1             0  ...                0   \n",
       "1          0       1262             0             1  ...                0   \n",
       "2          1       1786             1             0  ...                0   \n",
       "3          1       1717             1             0  ...                0   \n",
       "4          0       2198             1             0  ...                0   \n",
       "\n",
       "   HouseStyle_1.5Unf  HouseStyle_1Story  HouseStyle_2.5Fin  HouseStyle_2.5Unf  \\\n",
       "0                  0                  0                  0                  0   \n",
       "1                  0                  1                  0                  0   \n",
       "2                  0                  0                  0                  0   \n",
       "3                  0                  0                  0                  0   \n",
       "4                  0                  0                  0                  0   \n",
       "\n",
       "   HouseStyle_2Story  HouseStyle_SFoyer  HouseStyle_SLvl  PavedDrive_P  \\\n",
       "0                  1                  0                0             0   \n",
       "1                  0                  0                0             0   \n",
       "2                  1                  0                0             0   \n",
       "3                  1                  0                0             0   \n",
       "4                  1                  0                0             0   \n",
       "\n",
       "   PavedDrive_Y  \n",
       "0             1  \n",
       "1             1  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 28106.463641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear base learners\n",
    "Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as xgb.train().\n",
    "\n",
    "In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 when you used `xgb.cv()`). The key-value pair that defines the booster type (base model) you need is `\"booster\":\"gblinear\"`.\n",
    "\n",
    "Once you've created the model, you can use the `.train()` and `.predict()` methods of the model just like you've done in the past.\n",
    "\n",
    "Here, the data has already been split into training and testing sets, so you can dive right into creating the `DMatrix` objects required by the XGBoost learning API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:59:14] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE: 43489.479351\n"
     ]
    }
   ],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model quality\n",
    "It's now time to begin evaluating model quality.\n",
    "\n",
    "Here, you will compare the RMSE and MAE of a cross-validated XGBoost model on the Ames housing data. As in previous exercises, all necessary modules have been pre-loaded and the data is available in the DataFrame `df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform 4-fold cross-validation with 5 boosting rounds and `\"rmse\"` as the metric.\n",
    "\n",
    "* Extract and print the final boosting round RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:59:15] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:15] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:15] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:15] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0    141767.531250      429.448328   142980.433594    1193.798509\n",
      "1    102832.542969      322.473304   104891.396485    1223.159762\n",
      "2     75872.619140      266.472468    79478.937500    1601.344539\n",
      "3     57245.650390      273.624608    62411.921875    2220.149857\n",
      "4     44401.297851      316.422372    51348.280274    2963.377140\n",
      "4    51348.280274\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, adapt your code to compute the `\"mae\"` instead of the `\"rmse\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:59:16] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:16] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:16] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:16] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
      "0   127343.480468     668.337982  127633.978515   2404.001617\n",
      "1    89770.052735     456.957620   90122.503906   2107.912235\n",
      "2    63580.788085     263.407193   64278.563477   1887.565119\n",
      "3    45633.153321     151.884551   46819.167968   1459.819967\n",
      "4    33587.093750      86.998768   35670.646484   1140.607452\n",
      "4    35670.646484\n",
      "Name: test-mae-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and base learners in XGBoost\n",
    "* Regularization is a control on model complexity\n",
    "* We want the models to be both accurate and as simple as possible\n",
    "* Regularization params in XGBoost:\n",
    "    - gamma - minimum loss reduction allowed for a split to occur (higher values - lower splits)\n",
    "    - alpha - L1 regularization on leaf weights, larger values mean more regularization (leaf weights go to 0)\n",
    "    - lambda - L2 regularization on leaf weights (smoothly decrease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base learners in XGBoost\n",
    "* Linear Base Learners:\n",
    "    - Sum of linear terms\n",
    "    - Boosted model is weighted sum of linear models (thus is itself linear)\n",
    "    - Rarely used\n",
    "\n",
    "* Tree Base Learners:\n",
    "    - Decision tree\n",
    "    - Boosted model is weighted sum od decision trees (non-linear)\n",
    "    - almost exclusively used in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using regularization in XGBoost\n",
    "Having seen an example of l1 regularization in the video, you'll now vary the l2 regularization penalty - also known as `\"lambda\"` - and see its effect on overall model performance on the Ames housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:59:17] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:17] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:17] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:17] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:17] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:17] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Best rmse as a function of l2:\n",
      "    l2          rmse\n",
      "0    1  52275.357421\n",
      "1   10  57746.064453\n",
      "2  100  76624.625000\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "    params[\"lambda\"] = reg\n",
    "    \n",
    "    # Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing individual XGBoost trees\n",
    "Now that you've used XGBoost to both build and evaluate regression as well as classification models, you should get a handle on how to visually explore your models. Here, you will visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset.\n",
    "\n",
    "XGBoost has a `plot_tree()` function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the `plot_tree()` function along with the number of trees you want to plot using the `num_trees` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:59:18] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7d4e4f918643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Plot the first tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Plot the fifth tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the first tree\n",
    "xgb.plot_tree(xg_reg, num_trees=0)\n",
    "plt.show()\n",
    "\n",
    "# Plot the fifth tree\n",
    "xgb.plot_tree(xg_reg, num_trees=4)\n",
    "plt.show()\n",
    "\n",
    "# Plot the last tree sideways\n",
    "xgb.plot_tree(xg_reg, num_trees=9, rankdir=\"LR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing feature importances: What features are most important in my dataset\n",
    "Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model.\n",
    "\n",
    "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a `plot_importance()` function that allows you to do exactly this, and you'll get a chance to use it in this exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\": \"reg:linear\", \"max_depth\": 4}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning your XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the number of boosting rounds\n",
    "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use `xgb.cv()` inside a `for` loop and build one model per `num_boost_round` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:59:20] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:20] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:20] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:20] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:20] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:20] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:20] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:20] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:59:20] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.299479\n",
      "1                   10  34774.192708\n",
      "2                   15  32895.097656\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\": \"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, \n",
    "                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated boosting round selection using early_stopping\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within `xgb.cv()`. This is done using a technique called early stopping.\n",
    "\n",
    "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (`\"rmse\"` in our case) does not improve for a given number of rounds. Here you will use the `early_stopping_rounds` parameter in `xgb.cv()` with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when `num_boost_rounds` is reached, then early stopping does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:03:36] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:03:36] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:03:36] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0     141871.635417      403.636200   142640.651042     705.559164\n",
      "1     103057.036458       73.769561   104907.664062     111.112417\n",
      "2      75975.963542      253.726946    79262.054687     563.764349\n",
      "3      57420.531250      521.656754    61620.135417    1087.693857\n",
      "4      44552.955729      544.170190    50437.561198    1846.446330\n",
      "5      35763.949219      681.795751    43035.661458    2034.469207\n",
      "6      29861.464193      769.571238    38600.880208    2169.796232\n",
      "7      25994.676432      756.520565    36071.817708    2109.795430\n",
      "8      23306.835937      759.237670    34383.184896    1934.546688\n",
      "9      21459.769531      745.624998    33509.141276    1887.375284\n",
      "10     20148.721354      749.612769    32916.809245    1850.893589\n",
      "11     19215.382161      641.388291    32197.832682    1734.456935\n",
      "12     18627.389323      716.256596    31770.852865    1802.155484\n",
      "13     17960.694661      557.043073    31482.782552    1779.123767\n",
      "14     17559.736979      631.412969    31389.990234    1892.319927\n",
      "15     17205.712891      590.171393    31302.882162    1955.165902\n",
      "16     16876.571940      703.631755    31234.059896    1880.707172\n",
      "17     16597.662110      703.677609    31318.348308    1828.860391\n",
      "18     16330.460937      607.274494    31323.634766    1775.909567\n",
      "19     16005.972982      520.470911    31204.134766    1739.075860\n",
      "20     15814.301432      518.604477    31089.862630    1756.021674\n",
      "21     15493.405924      505.615987    31047.996094    1624.673955\n",
      "22     15270.734049      502.019527    31056.916015    1668.042812\n",
      "23     15086.381836      503.912899    31024.983724    1548.985354\n",
      "24     14917.608724      486.206468    30983.684896    1663.130201\n",
      "25     14709.590169      449.668438    30989.477214    1686.666560\n",
      "26     14457.286133      376.787666    30952.113281    1613.172332\n",
      "27     14185.567057      383.101961    31066.902344    1648.534310\n",
      "28     13934.066732      473.465449    31095.641276    1709.225654\n",
      "29     13749.644857      473.670302    31103.886719    1778.879529\n",
      "30     13549.836914      454.898923    30976.085938    1744.515079\n",
      "31     13413.485351      399.603618    30938.469401    1746.052597\n",
      "32     13275.916016      415.408786    30931.000651    1772.469906\n",
      "33     13085.878581      493.792860    30929.057292    1765.540659\n",
      "34     12947.181315      517.790039    30890.630208    1786.511479\n",
      "35     12846.027344      547.732372    30884.492839    1769.728719\n",
      "36     12702.379232      505.523221    30833.542318    1691.002985\n",
      "37     12532.244141      508.298241    30856.687500    1771.445978\n",
      "38     12384.055013      536.225042    30818.016927    1782.784630\n",
      "39     12198.444010      545.165502    30839.392578    1847.325597\n",
      "40     12054.583333      508.841412    30776.966146    1912.780507\n",
      "41     11897.036458      477.178360    30794.702474    1919.674832\n",
      "42     11756.221354      502.992395    30780.955078    1906.820029\n",
      "43     11618.846029      519.837153    30783.755859    1951.259331\n",
      "44     11484.080078      578.428250    30776.731120    1953.446309\n",
      "45     11356.552734      565.368794    30758.544271    1947.455425\n",
      "46     11193.558594      552.298906    30729.972005    1985.699316\n",
      "47     11071.315429      604.089960    30732.663411    1966.997809\n",
      "48     10950.777995      574.863209    30712.241536    1957.751573\n",
      "49     10824.865885      576.665756    30720.854818    1950.511520\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, early_stopping_rounds=10, num_boost_round=50,\n",
    "                    metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost's hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common tree tunable params\n",
    "* learning rate\n",
    "* gamma: min loss reduction to create new tree split\n",
    "* lambda, alpha: regularization params\n",
    "* max_depth: max depth per tree\n",
    "* subsample: % samples used per tree (low -> underfitting, high -> overfitting)\n",
    "* colsample_bytree:: @ of features per tree (low -> regularization, too high -> overfitting)\n",
    "* number of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common linear tunable params:\n",
    "* lambda, alpha\n",
    "* lambda_bias: L2 reg term on biad\n",
    "* number of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning eta\n",
    "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the `\"eta\"`, also known as the learning rate.\n",
    "\n",
    "The learning rate in XGBoost is a parameter that can range between `0` and `1`, with higher values of `\"eta\"` penalizing feature weights more strongly, causing much stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:12:41] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:12:41] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:12:41] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:12:41] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:12:41] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:12:41] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:12:41] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:12:41] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:12:41] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "     eta      best_rmse\n",
      "0  0.001  195736.411458\n",
      "1  0.010  179932.192708\n",
      "2  0.100   79759.411458\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, early_stopping_rounds=5, num_boost_round=10, \n",
    "                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning max_depth\n",
    "In this exercise, your job is to tune `max_depth`, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:15:30] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:15:30] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:15:30] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:15:30] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:15:30] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:15:30] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:15:30] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:15:30] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   max_depth     best_rmse\n",
      "0          2  37957.468750\n",
      "1          5  35596.599610\n",
      "2         10  36065.550782\n",
      "3         20  36739.578125\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, early_stopping_rounds=5, num_boost_round=10, \n",
    "                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning colsample_bytree\n",
    "Now, it's time to tune `\"colsample_bytree\"`. You've already seen this if you've ever worked with scikit-learn's `RandomForestClassifier` or `RandomForestRegressor`, where it just was called `max_features`. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between `0` and `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:17:24] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:17:24] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:17:24] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:17:24] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:17:24] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:17:24] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:17:24] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:17:24] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   colsample_bytree     best_rmse\n",
      "0               0.1  51386.576172\n",
      "1               0.5  36585.351562\n",
      "2               0.8  36093.660157\n",
      "3               1.0  35836.042969\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of grid search and random search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "* Search exhaustively over a given set of hyperparameters, once per set of hyperparameters\n",
    "* Number of models = number of distinct values per hyperparameter multiplied across each hyperparameter\n",
    "* Pick final model hyperparameter values that give best cross-validated evaluation metric value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search\n",
    "* Create a (possibly infinite) range of hyperparameter values per hyperparameter that you would like to search over\n",
    "* Set the number of iterations you would like for the random search to contin ue\n",
    "* During each iteration, randomly draw a value in the range of specified values for each hyperparameter searched over and train/evaluate a model with those hyperparameters\n",
    "* After you reach the max number of iterations, select the hyperparemeters configuration with the best evaluated score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 2, 'n_estimators': 50}\n",
      "Lowest RMSE found:  30355.698207097197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search with XGBoost\n",
    "Often, `GridSearchCV` can be really time consuming, so in practice, you may want to use `RandomizedSearchCV` instead, as you will do in this exercise. The good news is you only have to make a few modifications to your `GridSearchCV` code to do `RandomizedSearchCV`. The key difference is you have to specify a `param_distributions` parameter instead of a `param_grid` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 4}\n",
      "Lowest RMSE found:  29998.4522530019\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, estimator=gbm, scoring=\"neg_mean_squared_error\", n_iter=5, cv=4, verbose=1)\n",
    "\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits of grid search and random search\n",
    "\n",
    "* Grid Search\n",
    "    - number of models you must build with every additional new parameter grows very quickly\n",
    "* Random Search\n",
    "    - parameter space to explore can be massive\n",
    "    - randomly jumping throught the space looking for a \"best\" result becomes a waiting game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using XGBoost in pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* `LabelEncoder`: converts a categorical column of strings into integers\n",
    "* `OneHotEncoder`: takes the column of integers and encodes them as dummy variables\n",
    "\n",
    "* `DictVectorizer`: performs those two steps together (converts lists of feature mappings into vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"ames_unprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>...</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>...</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>548</td>\n",
       "      <td>Y</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>...</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "      <td>Y</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>...</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>Y</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>...</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>642</td>\n",
       "      <td>Y</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>...</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>836</td>\n",
       "      <td>Y</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Neighborhood BldgType HouseStyle  \\\n",
       "0          60       RL         65.0     8450      CollgCr     1Fam     2Story   \n",
       "1          20       RL         80.0     9600      Veenker     1Fam     1Story   \n",
       "2          60       RL         68.0    11250      CollgCr     1Fam     2Story   \n",
       "3          70       RL         60.0     9550      Crawfor     1Fam     2Story   \n",
       "4          60       RL         84.0    14260      NoRidge     1Fam     2Story   \n",
       "\n",
       "   OverallQual  OverallCond  YearBuilt  ...  GrLivArea  BsmtFullBath  \\\n",
       "0            7            5       2003  ...       1710             1   \n",
       "1            6            8       1976  ...       1262             0   \n",
       "2            7            5       2001  ...       1786             1   \n",
       "3            7            5       1915  ...       1717             1   \n",
       "4            8            5       2000  ...       2198             1   \n",
       "\n",
       "   BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  Fireplaces  GarageArea  \\\n",
       "0             0         2         1             3           0         548   \n",
       "1             1         2         0             3           1         460   \n",
       "2             0         2         1             3           1         608   \n",
       "3             0         1         0             3           1         642   \n",
       "4             0         2         1             4           1         836   \n",
       "\n",
       "   PavedDrive SalePrice  \n",
       "0           Y    208500  \n",
       "1           Y    181500  \n",
       "2           Y    223500  \n",
       "3           Y    140000  \n",
       "4           Y    250000  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 21)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Remodeled</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>0.476712</td>\n",
       "      <td>1515.463699</td>\n",
       "      <td>0.425342</td>\n",
       "      <td>0.057534</td>\n",
       "      <td>1.565068</td>\n",
       "      <td>0.382877</td>\n",
       "      <td>2.866438</td>\n",
       "      <td>0.613014</td>\n",
       "      <td>472.980137</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>0.499629</td>\n",
       "      <td>525.480383</td>\n",
       "      <td>0.518911</td>\n",
       "      <td>0.238753</td>\n",
       "      <td>0.550916</td>\n",
       "      <td>0.502885</td>\n",
       "      <td>0.815778</td>\n",
       "      <td>0.644666</td>\n",
       "      <td>213.804841</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1129.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>334.500000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1776.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5642.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MSSubClass  LotFrontage        LotArea  OverallQual  OverallCond  \\\n",
       "count  1460.000000  1201.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     56.897260    70.049958   10516.828082     6.099315     5.575342   \n",
       "std      42.300571    24.284752    9981.264932     1.382997     1.112799   \n",
       "min      20.000000    21.000000    1300.000000     1.000000     1.000000   \n",
       "25%      20.000000    59.000000    7553.500000     5.000000     5.000000   \n",
       "50%      50.000000    69.000000    9478.500000     6.000000     5.000000   \n",
       "75%      70.000000    80.000000   11601.500000     7.000000     6.000000   \n",
       "max     190.000000   313.000000  215245.000000    10.000000     9.000000   \n",
       "\n",
       "         YearBuilt    Remodeled    GrLivArea  BsmtFullBath  BsmtHalfBath  \\\n",
       "count  1460.000000  1460.000000  1460.000000   1460.000000   1460.000000   \n",
       "mean   1971.267808     0.476712  1515.463699      0.425342      0.057534   \n",
       "std      30.202904     0.499629   525.480383      0.518911      0.238753   \n",
       "min    1872.000000     0.000000   334.000000      0.000000      0.000000   \n",
       "25%    1954.000000     0.000000  1129.500000      0.000000      0.000000   \n",
       "50%    1973.000000     0.000000  1464.000000      0.000000      0.000000   \n",
       "75%    2000.000000     1.000000  1776.750000      1.000000      0.000000   \n",
       "max    2010.000000     1.000000  5642.000000      3.000000      2.000000   \n",
       "\n",
       "          FullBath     HalfBath  BedroomAbvGr   Fireplaces   GarageArea  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1460.000000  1460.000000   \n",
       "mean      1.565068     0.382877      2.866438     0.613014   472.980137   \n",
       "std       0.550916     0.502885      0.815778     0.644666   213.804841   \n",
       "min       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
       "25%       1.000000     0.000000      2.000000     0.000000   334.500000   \n",
       "50%       2.000000     0.000000      3.000000     1.000000   480.000000   \n",
       "75%       2.000000     1.000000      3.000000     1.000000   576.000000   \n",
       "max       3.000000     2.000000      8.000000     3.000000  1418.000000   \n",
       "\n",
       "           SalePrice  \n",
       "count    1460.000000  \n",
       "mean   180921.195890  \n",
       "std     79442.502883  \n",
       "min     34900.000000  \n",
       "25%    129975.000000  \n",
       "50%    163000.000000  \n",
       "75%    214000.000000  \n",
       "max    755000.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical columns I: LabelEncoder\n",
    "Now that you've seen what will need to be done to get the housing data ready for XGBoost, let's go through the process step-by-step.\n",
    "\n",
    "First, you will need to fill in missing values - as you saw previously, the column `LotFrontage` has many missing values. Then, you will need to encode any categorical columns in the dataset using one-hot encoding so that they are encoded numerically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n",
      "0       RL      CollgCr     1Fam     2Story          Y\n",
      "1       RL      Veenker     1Fam     1Story          Y\n",
      "2       RL      CollgCr     1Fam     2Story          Y\n",
      "3       RL      Crawfor     1Fam     2Story          Y\n",
      "4       RL      NoRidge     1Fam     2Story          Y\n",
      "   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
      "0         3             5         0           5           2\n",
      "1         3            24         0           2           2\n",
      "2         3             5         0           5           2\n",
      "3         3             6         0           5           2\n",
      "4         3            15         0           5           2\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df2.LotFrontage.fillna(0)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df2.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df2.columns[categorical_mask].tolist()\n",
    "\n",
    "# Print the head of the categorical columns\n",
    "print(df2[categorical_columns].head())\n",
    "\n",
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "df2[categorical_columns] = df2[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "print(df2[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical columns II: OneHotEncoder\n",
    "Okay - so you have your categorical columns encoded numerically. Can you now move onto using pipelines and XGBoost? Not yet! In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using `LabelEncoder`, the `CollgCr` `Neighborhood` was encoded as `5`, while the `Veenker` `Neighborhood` was encoded as `24`, and `Crawfor` as `6`. Is `Veenker` \"greater\" than `Crawfor` and `CollgCr`? No - and allowing the model to assume this natural ordering may result in poor performance.\n",
    "\n",
    "As a result, there is another step needed: You have to apply a one-hot encoding to create binary, or \"dummy\" variables. You can do this using scikit-learn's OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deprecated in the newest version of sklearn\n",
    "\n",
    "# Import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "df_encoded2 = ohe.fit_transform(df2)\n",
    "\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print(df_encoded2[:5, :])\n",
    "\n",
    "# Print the shape of the original DataFrame\n",
    "print(df2.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(df_encoded2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical columns III: DictVectorizer\n",
    "Alright, one final trick before you dive into pipelines. The two step process you just went through - `LabelEncoder` followed `by OneHotEncoder` - can be simplified by using a `DictVectorizer`.\n",
    "\n",
    "Using a `DictVectorizer` on a DataFrame that has been converted to a dictionary allows you to get label encoding as well as one-hot encoding in one go.\n",
    "\n",
    "Your task is to work through this strategy in this exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 2.000e+00 5.480e+02\n",
      "  1.710e+03 1.000e+00 5.000e+00 8.450e+03 6.500e+01 6.000e+01 3.000e+00\n",
      "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 0.000e+00 2.085e+05 2.003e+03]\n",
      " [3.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.000e+00 4.600e+02\n",
      "  1.262e+03 0.000e+00 2.000e+00 9.600e+03 8.000e+01 2.000e+01 3.000e+00\n",
      "  2.400e+01 8.000e+00 6.000e+00 2.000e+00 0.000e+00 1.815e+05 1.976e+03]\n",
      " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 6.080e+02\n",
      "  1.786e+03 1.000e+00 5.000e+00 1.125e+04 6.800e+01 6.000e+01 3.000e+00\n",
      "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 2.235e+05 2.001e+03]\n",
      " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 1.000e+00 6.420e+02\n",
      "  1.717e+03 0.000e+00 5.000e+00 9.550e+03 6.000e+01 7.000e+01 3.000e+00\n",
      "  6.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 1.400e+05 1.915e+03]\n",
      " [4.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 8.360e+02\n",
      "  2.198e+03 1.000e+00 5.000e+00 1.426e+04 8.400e+01 6.000e+01 3.000e+00\n",
      "  1.500e+01 5.000e+00 8.000e+00 2.000e+00 0.000e+00 2.500e+05 2.000e+03]]\n",
      "{'MSSubClass': 12, 'MSZoning': 13, 'LotFrontage': 11, 'LotArea': 10, 'Neighborhood': 14, 'BldgType': 1, 'HouseStyle': 9, 'OverallQual': 16, 'OverallCond': 15, 'YearBuilt': 20, 'Remodeled': 18, 'GrLivArea': 7, 'BsmtFullBath': 2, 'BsmtHalfBath': 3, 'FullBath': 5, 'HalfBath': 8, 'BedroomAbvGr': 0, 'Fireplaces': 4, 'GarageArea': 6, 'PavedDrive': 17, 'SalePrice': 19}\n"
     ]
    }
   ],
   "source": [
    "# Import DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Convert df into a dictionary: df_dict\n",
    "df_dict2 = df2.to_dict(\"records\")\n",
    "\n",
    "# Create the DictVectorizer object: dv\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Apply dv on df: df_encoded\n",
    "df_encoded2 = dv.fit_transform(df_dict2)\n",
    "\n",
    "# Print the resulting first five rows\n",
    "print(df_encoded2[:5,:])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(dv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing within a pipeline\n",
    "Now that you've seen what steps need to be taken individually to properly process the Ames housing data, let's use the much cleaner and more succinct `DictVectorizer` approach and put it alongside an `XGBoostRegressor` inside of a scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ohe_onestep', DictVectorizer(sparse=False)),\n",
       "                ('xgb_model',\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='',\n",
       "                              learning_rate=0.300000012, max_delta_step=0,\n",
       "                              max_depth=6, min_child_weight=1, missing=nan,\n",
       "                              monotone_constraints='()', n_estimators=100,\n",
       "                              n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                              subsample=1, tree_method='exact',\n",
       "                              validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "xgb_pipeline.fit(X.to_dict(\"records\"), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating XGBoost into pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `sklearn_pandas`:\n",
    "    - `DataFrameMapper` - interoperability between `pandas` and `scikit-learn`\n",
    "    - `CategoricalImputer` - allow for imputation of categorical variables before conversion to integers\n",
    "* `sklearn.preprocessing`:\n",
    "    - `Imputer` - native imputation of numerical columns in scikit-learn\n",
    " * `sklearn.pipeline`:\n",
    "     - `FeatureUnion` - combine multiple pipelines of features into a single pipeline of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validating your XGBoost model\n",
    "In this exercise, you'll go one step further by using the pipeline you've created to preprocess and cross-validate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:52:32] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:52:32] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:52:33] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:52:33] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:52:33] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:52:33] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:52:33] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:52:33] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:52:33] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:52:34] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "10-fold RMSE:  27840.01103862237\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(\"records\"), y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kidney disease case study I: Categorical Imputer\n",
    "You'll now continue your exploration of using pipelines with a dataset that requires significantly more wrangling. The chronic kidney disease dataset contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features.\n",
    "\n",
    "As Sergey mentioned in the video, you'll be introduced to a new library, sklearn_pandas, that allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, you'll be able to impute missing categorical values directly using the `Categorical_Imputer()` class in `sklearn_pandas`, and the `DataFrameMapper()` class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.\n",
    "\n",
    "We've also created a transformer called a `Dictifier` that encapsulates converting a DataFrame using `.to_dict(\"records\")` without you having to do it explicitly (and so that it works in a pipeline). Finally, we've also provided the list of feature names in `kidney_feature_names`, the target name in `kidney_target_name`, the features in `X`, and the target in `y`.\n",
    "\n",
    "In this exercise, your task is to apply the CategoricalImputer to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments `input_df=True` and `df_out=True`? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a `numpy` array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with `numpy` arrays, not `pandas` DataFrames, even though their basic indexing interfaces are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSSubClass              0\n",
      "LotFrontage             0\n",
      "LotArea                 0\n",
      "OverallQual             0\n",
      "OverallCond             0\n",
      "YearBuilt               0\n",
      "Remodeled               0\n",
      "GrLivArea               0\n",
      "BsmtFullBath            0\n",
      "BsmtHalfBath            0\n",
      "FullBath                0\n",
      "HalfBath                0\n",
      "BedroomAbvGr            0\n",
      "Fireplaces              0\n",
      "GarageArea              0\n",
      "MSZoning_FV             0\n",
      "MSZoning_RH             0\n",
      "MSZoning_RL             0\n",
      "MSZoning_RM             0\n",
      "Neighborhood_Blueste    0\n",
      "Neighborhood_BrDale     0\n",
      "Neighborhood_BrkSide    0\n",
      "Neighborhood_ClearCr    0\n",
      "Neighborhood_CollgCr    0\n",
      "Neighborhood_Crawfor    0\n",
      "Neighborhood_Edwards    0\n",
      "Neighborhood_Gilbert    0\n",
      "Neighborhood_IDOTRR     0\n",
      "Neighborhood_MeadowV    0\n",
      "Neighborhood_Mitchel    0\n",
      "Neighborhood_NAmes      0\n",
      "Neighborhood_NPkVill    0\n",
      "Neighborhood_NWAmes     0\n",
      "Neighborhood_NoRidge    0\n",
      "Neighborhood_NridgHt    0\n",
      "Neighborhood_OldTown    0\n",
      "Neighborhood_SWISU      0\n",
      "Neighborhood_Sawyer     0\n",
      "Neighborhood_SawyerW    0\n",
      "Neighborhood_Somerst    0\n",
      "Neighborhood_StoneBr    0\n",
      "Neighborhood_Timber     0\n",
      "Neighborhood_Veenker    0\n",
      "BldgType_2fmCon         0\n",
      "BldgType_Duplex         0\n",
      "BldgType_Twnhs          0\n",
      "BldgType_TwnhsE         0\n",
      "HouseStyle_1.5Unf       0\n",
      "HouseStyle_1Story       0\n",
      "HouseStyle_2.5Fin       0\n",
      "HouseStyle_2.5Unf       0\n",
      "HouseStyle_2Story       0\n",
      "HouseStyle_SFoyer       0\n",
      "HouseStyle_SLvl         0\n",
      "PavedDrive_P            0\n",
      "PavedDrive_Y            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "# from sklearn_pandas import CategoricalImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature], SimpleImputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, SimpleImputer()) for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kidney disease case study II: Feature Union\n",
    "Having separately imputed numeric as well as categorical columns, your task is now to use scikit-learn's FeatureUnion to concatenate their results, which are contained in two separate transformer objects - `numeric_imputation_mapper`, and `categorical_imputation_mapper`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kidney disease case study III: Full pipeline\n",
    "It's time to piece together all of the transforms along with an `XGBClassifier` to build the full pipeline!\n",
    "\n",
    "Besides the `numeric_categorical_union` that you created in the previous exercise, there are two other transforms needed: the `Dictifier(`) transform which we created for you, and the `DictVectorizer()`.\n",
    "\n",
    "After creating the pipeline, your task is to cross-validate it to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
    "                    ])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, kidney_data, y, scoring=\"roc_auc\", cv=3)\n",
    "\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning XGBoost hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together\n",
    "Alright, it's time to bring together everything you've learned so far! In this final exercise of the course, you will combine your work from the previous exercises into one end-to-end XGBoost pipeline to really cement your understanding of preprocessing and pipelines in XGBoost.\n",
    "\n",
    "Your work from the previous 3 exercises, where you preprocessed the data and set up your pipeline, has been pre-loaded. Your job is to perform a randomized search and identify the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'clf__max_depth': np.arange(3, 10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline, param_distributions=gbm_param_grid, \n",
    "                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_roc_auc.best_score_)\n",
    "print(randomized_roc_auc.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nest steps\n",
    "- Using XGBoost for ranking/recommendation\n",
    "- Using more sophisticated hyperparameters tuning strategies for tuning XGBoost models (Bayesian Optimization)\n",
    "- Using XGBoost as a part of an ensemble of other models for regression/classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
